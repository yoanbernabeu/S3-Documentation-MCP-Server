---
title: Embedding Providers
description: Choose between Ollama (local) and OpenAI (cloud) for embeddings
---

The S3 Documentation MCP server supports two embedding providers: **Ollama** (local) and **OpenAI** (cloud).

## Comparison

| Feature | Ollama | OpenAI |
|---------|--------|--------|
| **Cost** | Free | ~$0.00002/1K tokens |
| **Privacy** | 100% local | Data sent to OpenAI |
| **Offline** | ‚úÖ Yes | ‚ùå No |
| **Accuracy** | Good | Excellent |
| **Multilingual** | Good | Excellent |
| **Setup** | Install + model download | API key only |
| **Resources** | Local CPU/GPU | Cloud-based |

## Ollama (Local)

**Recommended for:** Local development, privacy-conscious deployments, offline usage

### Setup

1. Install Ollama from [https://ollama.ai](https://ollama.ai)

2. Pull the embedding model:
   ```bash
   ollama pull nomic-embed-text
   ```

3. Configure in `.env`:
   ```bash
   EMBEDDING_PROVIDER=ollama
   OLLAMA_BASE_URL=http://localhost:11434
   OLLAMA_EMBEDDING_MODEL=nomic-embed-text
   ```

### Docker Configuration

When using Docker, use `host.docker.internal` to access Ollama running on the host:

```bash
OLLAMA_BASE_URL=http://host.docker.internal:11434
```

Or in docker-compose.yml:

```yaml
environment:
  - OLLAMA_BASE_URL=http://host.docker.internal:11434
```

### Pros

- ‚úÖ **Free**: No API costs, unlimited usage
- ‚úÖ **Private**: All data stays on your machine
- ‚úÖ **Offline**: Works without internet connection
- ‚úÖ **Fast**: Direct local API calls
- ‚úÖ **No Rate Limits**: Process as much as you want

### Cons

- ‚ö†Ô∏è Requires Ollama installation and model download (~270MB)
- ‚ö†Ô∏è Uses local CPU/GPU resources
- ‚ö†Ô∏è Slightly lower accuracy than cloud models

### About nomic-embed-text

The `nomic-embed-text` model is:

- **Dimension**: 768
- **Size**: ~270MB
- **Performance**: Excellent for English, good for other languages
- **Speed**: Very fast on modern CPUs
- **License**: Apache 2.0 (fully open-source)

## OpenAI (Cloud)

**Recommended for:** Production deployments, multilingual content, maximum accuracy

### Setup

1. Get an API key from [OpenAI Platform](https://platform.openai.com/api-keys)

2. Add credits to your account

3. Configure in `.env`:
   ```bash
   EMBEDDING_PROVIDER=openai
   OPENAI_API_KEY=sk-...your-key...
   OPENAI_EMBEDDING_MODEL=text-embedding-3-small
   ```

### Model Options

#### text-embedding-3-small (Recommended)

```bash
OPENAI_EMBEDDING_MODEL=text-embedding-3-small
```

- **Dimensions**: 1536
- **Cost**: ~$0.00002/1K tokens
- **Performance**: High accuracy
- **Best for**: Most use cases, cost-sensitive deployments

#### text-embedding-3-large

```bash
OPENAI_EMBEDDING_MODEL=text-embedding-3-large
```

- **Dimensions**: 3072
- **Cost**: ~$0.00013/1K tokens
- **Performance**: Maximum accuracy
- **Best for**: Multilingual content, maximum precision

### Pros

- ‚úÖ **High Accuracy**: State-of-the-art embeddings
- ‚úÖ **Multilingual**: Excellent support for 20+ languages
- ‚úÖ **No Local Resources**: Runs entirely in the cloud
- ‚úÖ **Lower Latency**: Fast API responses
- ‚úÖ **Scalable**: No local hardware limits

### Cons

- ‚ö†Ô∏è Requires API key and credits
- ‚ö†Ô∏è Data sent to OpenAI servers
- ‚ö†Ô∏è Requires internet connection
- ‚ö†Ô∏è Rate limits apply (though very generous)

### Cost Estimation

Typical documentation indexing costs:

| Documentation Size | Tokens (approx.) | Cost (text-embedding-3-small) |
|-------------------|------------------|-------------------------------|
| 100 pages | ~250K tokens | ~$0.005 |
| 500 pages | ~1.25M tokens | ~$0.025 |
| 1000 pages | ~2.5M tokens | ~$0.05 |

**Search costs are negligible** (~$0.00001 per query).

## Fallback Behavior

If you set `EMBEDDING_PROVIDER=openai` but don't provide a valid `OPENAI_API_KEY`, the server will:

1. ‚ö†Ô∏è Log a warning
2. üîÑ Automatically fall back to Ollama (if configured)
3. ‚ùå Fail to start if neither provider is available

This ensures the server can always start with a working configuration.

## Switching Providers

:::caution[Important]
When switching between providers, you **must rebuild your vector index** because embeddings are not compatible:

```bash
# Delete existing index
rm -rf ./data/hnswlib-store

# Restart the server (will rebuild with new provider)
npm start  # or docker-compose restart
```
:::

## Performance Comparison

Real-world performance on a typical documentation set (500 pages):

| Provider | Indexing Time | Search Time | Accuracy |
|----------|---------------|-------------|----------|
| Ollama (nomic-embed-text) | ~5 min | ~50ms | Good ‚≠ê‚≠ê‚≠ê‚≠ê |
| OpenAI (text-embedding-3-small) | ~2 min | ~100ms | Excellent ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| OpenAI (text-embedding-3-large) | ~2 min | ~100ms | Best ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |

*Times measured on: M1 MacBook Pro (Ollama), standard network connection (OpenAI)*

## Recommendations

### For Local Development
```bash
EMBEDDING_PROVIDER=ollama
```
Fast, free, and private. Perfect for testing and iteration.

### For Production (English-only)
```bash
EMBEDDING_PROVIDER=openai
OPENAI_EMBEDDING_MODEL=text-embedding-3-small
```
Great accuracy at minimal cost.

### For Production (Multilingual)
```bash
EMBEDDING_PROVIDER=openai
OPENAI_EMBEDDING_MODEL=text-embedding-3-large
```
Maximum accuracy across all languages.

### For Privacy-Critical Deployments
```bash
EMBEDDING_PROVIDER=ollama
```
Keep all data on-premises.

## Next Steps

- [Configure environment variables](/S3-Documentation-MCP-Server/configuration/environment-variables/)
- [Set up synchronization](/S3-Documentation-MCP-Server/configuration/sync-modes/)
- [Start using MCP tools](/S3-Documentation-MCP-Server/usage/mcp-tools/)

