# S3 Documentation MCP Server

[![CI](https://github.com/yoanbernabeu/s3-doc-mcp/actions/workflows/ci.yml/badge.svg)](https://github.com/yoanbernabeu/s3-doc-mcp/actions/workflows/ci.yml)
[![Docker Build](https://github.com/yoanbernabeu/s3-doc-mcp/actions/workflows/docker-build.yml/badge.svg)](https://github.com/yoanbernabeu/s3-doc-mcp/actions/workflows/docker-build.yml)

A **Model Context Protocol (MCP)** server that enables LLMs (Claude, GPT, etc.) to **semantically search** through Markdown documentation stored on S3-compatible services using **local RAG** (Retrieval Augmented Generation).

## ğŸ“– What is this?

This server acts as a **bridge** between your LLM and your S3-stored documentation:

1. **ğŸ“¥ Indexes**: Automatically scans and downloads Markdown files (`.md`) from your S3 bucket
2. **ğŸ”ª Chunks**: Splits documents into smaller, semantically meaningful chunks
3. **ğŸ§® Vectorizes**: Generates embeddings using **Ollama** (100% local, no API costs)
4. **ğŸ’¾ Stores**: Saves vectors in an optimized **HNSWLib** index for ultra-fast similarity search
5. **ğŸ” Searches**: When your LLM needs information, it performs semantic search to find the most relevant passages
6. **ğŸ”„ Syncs**: Intelligently detects changes (new, modified, or deleted files) and updates the index accordingly

```mermaid
graph TB
    S3["ğŸ“¦ S3 Bucket<br/>(Markdown Documentation)"]
    Server["ğŸ–¥ï¸ S3 Doc MCP Server<br/>â€¢ Chunks docs<br/>â€¢ Vectorizes<br/>â€¢ Stores locally"]
    LLM["ğŸ¤– LLM (Claude/GPT)<br/>search_documentation('How to...?')<br/>â†“<br/>Gets relevant context"]
    
    S3 -->|"1. Scan & Sync"| Server
    Server -->|"2. MCP Tools"| LLM
    
    style S3 fill:#e1f5ff
    style Server fill:#fff3e0
    style LLM fill:#f3e5f5
```

## ğŸ¯ Key Features

### ğŸ”Œ Universal S3 Compatibility
- **AWS S3**: Native support
- **MinIO**: Self-hosted, on-premise
- **Scaleway Object Storage**: European cloud provider
- **DigitalOcean Spaces**: Simple cloud storage
- **Cloudflare R2**: Zero egress fees
- **Wasabi**: Hot cloud storage
- **Any S3-compatible API**: Custom endpoints supported

### ğŸ§  Local RAG with Zero API Costs
- **Local embeddings** via Ollama (`nomic-embed-text`)
- **No external API calls** for vectorization
- **Private & secure**: Your data never leaves your infrastructure
- **Fast**: HNSWLib provides near-instantaneous similarity search

### ğŸ”„ Intelligent Synchronization
- **Incremental sync**: Only processes changed files (via ETag comparison)
- **Automatic detection**: New, modified, and deleted files
- **Startup sync**: Index updates automatically when server starts
- **Periodic sync**: Optional scheduled re-indexing
- **Manual refresh**: On-demand via MCP tool

### ğŸ› ï¸ MCP Tools

#### `search_documentation`
Performs semantic search across your indexed documentation.

**Input:**
```json
{
  "query": "How do I configure authentication?",
  "max_results": 4
}
```

**Output:**
- Relevant document chunks
- Source file paths
- Similarity scores
- Formatted context ready for LLM consumption

#### `refresh_index`
Triggers index synchronization with S3.

**Input:**
```json
{
  "force": false  // true = full reindex, false = incremental
}
```

**Output:**
- Sync metrics (documents added/modified/deleted)
- Duration
- Error count

## ğŸ’¡ Use Cases

- **ğŸ“š Technical Documentation**: Make your product docs queryable by AI assistants
- **ğŸ¢ Internal Knowledge Base**: Enable AI-powered search across company wikis
- **ğŸ“– API Documentation**: Help developers find relevant API information
- **ğŸ“ Research Papers**: Search through academic documentation
- **ğŸ’¬ Customer Support**: Build AI assistants that reference your help center
- **ğŸ“ Educational Content**: Create AI tutors with access to course materials

## ğŸ”§ How It Works

### Architecture Overview

```mermaid
graph TD
    S3["ğŸ“¦ S3 Bucket<br/>Markdown Files"]
    
    S3Loader["S3Loader<br/>â€¢ Lists .md files<br/>â€¢ Downloads content<br/>â€¢ Tracks ETags"]
    
    SyncService["SyncService<br/>â€¢ Detects changes<br/>â€¢ Incremental sync<br/>â€¢ Full reindex"]
    
    VectorStore["HNSWVectorStore<br/>1ï¸âƒ£ Text Splitting (1000 chars/chunk)<br/>2ï¸âƒ£ Vectorization (Ollama)<br/>3ï¸âƒ£ Vector Storage (HNSWLib)<br/>4ï¸âƒ£ Persistence (./data/)"]
    
    HTTPServer["ğŸŒ HTTP Server<br/>Express + MCP<br/>POST /mcp"]
    
    Tools["ğŸ› ï¸ MCP Tools<br/>â€¢ search_documentation<br/>â€¢ refresh_index"]
    
    LLM["ğŸ¤– LLM Client<br/>Claude, GPT, etc.<br/>Semantic Search"]
    
    S3 -->|Load| S3Loader
    S3Loader -->|Documents| SyncService
    SyncService -->|Index| VectorStore
    VectorStore -->|Vectors| HTTPServer
    HTTPServer -->|Expose| Tools
    Tools -->|MCP Protocol| LLM
    
    style S3 fill:#e3f2fd
    style S3Loader fill:#fff3e0
    style SyncService fill:#fff3e0
    style VectorStore fill:#e8f5e9
    style HTTPServer fill:#f3e5f5
    style Tools fill:#f3e5f5
    style LLM fill:#fce4ec
```

### Data Flow

#### 1ï¸âƒ£ **Indexing Process**

```mermaid
graph LR
    A["ğŸ“„ Markdown File<br/>(S3)"] --> B["ğŸ”ª Split into chunks<br/>RecursiveCharacterTextSplitter"]
    B --> C["ğŸ§® Generate embeddings<br/>Ollama: nomic-embed-text"]
    C --> D["ğŸ’¾ Store vectors<br/>HNSWLib + cosine"]
    D --> E["ğŸ’½ Save to disk<br/>./data/hnswlib-store/"]
    
    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#e8f5e9
    style D fill:#f3e5f5
    style E fill:#fce4ec
```

#### 2ï¸âƒ£ **Search Process**

```mermaid
graph LR
    A["â“ User Query<br/>'How to configure S3?'"] --> B["ğŸ§® Vectorize query<br/>Ollama"]
    B --> C["ğŸ” Similarity search<br/>HNSWLib"]
    C --> D["ğŸ¯ Filter by threshold<br/>score >= 0.5"]
    D --> E["ğŸ“‹ Return top K results<br/>+ sources"]
    
    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#e8f5e9
    style D fill:#f3e5f5
    style E fill:#fce4ec
```

#### 3ï¸âƒ£ **Sync Process**

```mermaid
graph LR
    A["ğŸ“‹ List S3 files<br/>(.md only)"] --> B["âš–ï¸ Compare with<br/>.sync-state.json"]
    B --> C["ğŸ” Detect changes<br/>New | Modified | Deleted"]
    C --> D["âš¡ Apply changes<br/>to vector store"]
    D --> E["ğŸ’¾ Save state<br/>+ vector index"]
    
    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#e8f5e9
    style D fill:#f3e5f5
    style E fill:#fce4ec
```

### Technical Stack

- **MCP SDK**: `@modelcontextprotocol/sdk` - Model Context Protocol implementation
- **S3 Client**: `@aws-sdk/client-s3` - Universal S3 access
- **Vector Store**: `hnswlib-node` - Fast approximate nearest neighbor search
- **Embeddings**: `@langchain/community` with Ollama integration
- **Text Processing**: `@langchain/textsplitters` - Semantic chunking
- **Server**: Express.js with MCP HTTP transport

### Storage Structure

```
./data/
  â”œâ”€â”€ hnswlib-store/
  â”‚   â”œâ”€â”€ hnswlib.index       # Vector index (binary)
  â”‚   â”œâ”€â”€ docstore.json        # Document metadata
  â”‚   â””â”€â”€ args.json            # HNSWLib configuration
  â”‚
  â””â”€â”€ .sync-state.json         # Sync state tracking
      {
        "lastSyncDate": "2025-01-11T...",
        "documents": {
          "docs/intro.md": {
            "etag": "abc123...",
            "chunkCount": 5,
            "status": "indexed"
          }
        }
      }
```

## ğŸ“‹ Prerequisites

- **Node.js** >= 18.0.0
- **Ollama** installed and running
  ```bash
  brew install ollama  # macOS
  ollama serve
  ollama pull nomic-embed-text
  ```
- **S3 (or compatible)**: Read access to bucket
  - âœ… AWS S3
  - âœ… MinIO (self-hosted)
  - âœ… Scaleway Object Storage
  - âœ… DigitalOcean Spaces
  - âœ… Cloudflare R2
  - âœ… Wasabi
  - âœ… Any other S3-compatible provider

## ğŸš€ Installation

```bash
# Install dependencies
npm install

# Copy and configure environment variables
cp env.example .env
# Edit .env with your AWS credentials and configuration

# Build
npm run build
```

## ğŸ”§ Configuration

See `env.example` for all available environment variables.

## ğŸ“¦ Usage

### Development

```bash
npm run dev
```

### Production

```bash
npm run build
npm start
```

### ğŸ³ Docker

#### Build and Run

```bash
# Copy and configure environment variables
cp env.example .env
# Edit .env with your configuration

# Build and start with Docker Compose
docker compose up -d

# View logs
docker compose logs -f

# Stop
docker compose down
```

#### Configuration

The Docker setup expects:
- **Ollama** running externally on the host machine at `http://host.docker.internal:11434`
- **S3** (or compatible) service accessible from the container

**Note**: The Ollama URL is hardcoded to `http://host.docker.internal:11434` in the `compose.yaml` file. If your Ollama instance runs on a different host or port, you'll need to modify the `OLLAMA_BASE_URL` value directly in the `compose.yaml` file.

#### Volumes

The vector store data is persisted in a Docker volume named `vector-store-data` to avoid re-indexing on container restart.

## ğŸ“ License

MIT

## ğŸ‘¤ Author

Yoan Bernabeu

